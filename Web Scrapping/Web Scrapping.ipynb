{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "url = 'https://web.archive.org/web/20171117230616/http://lanyrd.com/speakers/'\n",
    "slide_url='https://web.archive.org/web/20171117230609/http://lanyrd.com/slides/'\n",
    "response = requests.get(url)\n",
    "slide_response = requests.get(slide_url)\n",
    "\n",
    "soup = bs(response.content, 'html.parser')\n",
    "slide = bs(slide_response.content, 'html.parser')\n",
    "sld= slide.find_all('div',{'class':\"coverage-item coverage-slides\"})\n",
    "all =soup.find_all(\"span\",{\"class\":\"name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img=soup.find_all(\"div\",{\"class\":\"avatar avatar-med\"})\n",
    "x=soup.find_all(\"div\",{\"avatar avatar-profile\"})\n",
    "for i in range(len(x)):\n",
    "    img.append(x[i])\n",
    "l=[]\n",
    "for i in range(len(img)): \n",
    "    d={}\n",
    "    k={}\n",
    "    name=[]\n",
    "    date=[]\n",
    "    d['Speaker']=all[i].find(\"a\").text\n",
    "    d['Display Image url'] = img[i].find(\"img\",attrs={\"src\" : re.compile(\"^https://\")}).get(\"src\").replace(\"\\'\",\"\")\n",
    "    \n",
    "    pro_url= ('https://web.archive.org'+img[i].find('a',attrs={'href': re.compile(\"^/web\")}).get('href'))\n",
    "    d['Website']=pro_url\n",
    "    profile =requests.get(pro_url)\n",
    "    pro_soup=bs(profile.content,\"html.parser\")\n",
    "    \n",
    "    pro_video= pro_soup.find_all('h3',{\"class\":\"title\"})\n",
    "    \n",
    "    pro_title=pro_soup.find_all('div',{\"class\":\"big-profile\"})\n",
    "    twitter=pro_soup.find_all('p',{\"class\":\"twitter icon\"})\n",
    "    facebook=pro_soup.find_all('p',{\"class\":\"facebook icon\"})\n",
    "    linkedin=pro_soup.find_all('p',{'class':\"linkedin icon\"})\n",
    "    \n",
    "    twitter1=pro_soup.find_all('span',{\"class\":\"name\"})\n",
    "    \n",
    "    event=pro_soup.find_all('li',{\"class\":\"conference vevent interactive-listing\"})\n",
    "    try:\n",
    "        d['Location']=(pro_title[0].find(\"p\",{\"class\":\"location\"}).text).replace('\\n','').replace(\"in\",\",\")\n",
    "        d['Title']=(pro_title[0].find(\"p\",{\"class\":\"tagline\"}).text).replace('\\n','').replace(\" \",\"\")   \n",
    "    except:\n",
    "        d['Location']=None\n",
    "        d['Title']='Independent designer; GIF sommelier. Started that whole “responsive web design” thing. (@RWD) Has a bukk.it. ⭐️ &gt; ❤️.'\n",
    "    \n",
    "    try:\n",
    "        sur= sld[i].find(\"p\",{\"class\":\"meta\"}).text.replace('at',',').replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"from \",\"\")\n",
    "    except:\n",
    "        sur=None\n",
    "    \n",
    "    try:\n",
    "        surl =(sld[i].find(\"a\",attrs={'rel': re.compile(\"^nofollow\")}).get('href'))\n",
    "    except:\n",
    "        surl=None\n",
    "    \n",
    "    try:\n",
    "        d['Linkedin'] =(linkedin[0].find(\"a\",attrs={'rel': re.compile(\"^me\")}).get('href'))\n",
    "    except:\n",
    "        d['Linkedin']=None\n",
    "    try:    \n",
    "        d['Twitter'] =(twitter[0].find(\"a\",attrs={'rel': re.compile(\"^me\")}).get('href'))\n",
    "    except:\n",
    "        try:\n",
    "            d['Twitter']=(twitter1[0].find(\"a\",attrs={'rel': re.compile(\"^me\")}).get('href'))\n",
    "        except:\n",
    "            d['Twitter']=None\n",
    "    try:    \n",
    "        d['Facebook'] =(facebook[0].find(\"a\",attrs={'rel': re.compile(\"^me\")}).get('href'))\n",
    "    except:\n",
    "        d['Facebook']=None\n",
    "        \n",
    "    for j in range(len(event)):\n",
    "        try:\n",
    "            name.append(event[j].find(\"a\",{\"class\":\"summary url\"}).text)\n",
    "            name.append(event[j].find(\"p\",{\"class\":\"date\"}).text)\n",
    "            name.append(event[j].find(\"p\",{\"class\":\"location\"}).text.replace('\\t','').replace('\\n',' '))\n",
    "        except:\n",
    "            name.append('None')\n",
    "    date.append(name)\n",
    "    k['Name','City,Country','Date']=date\n",
    "    d['Attended Event']=k\n",
    "    d['Slide Decks']= sur +\", \"+ surl \n",
    "    \n",
    "    try:\n",
    "        video_url= ('https://web.archive.org'+pro_video[0].find('a',attrs={'href': re.compile(\"^/web\")}).get('href'))\n",
    "    except:\n",
    "        None\n",
    "      \n",
    "    Vid =requests.get(video_url)\n",
    "    video_soup=bs(Vid.content,\"html.parser\")\n",
    "    video=video_soup.find_all('div',{\"class\":\"coverage-item coverage-video\"})\n",
    "    video1=video_soup.find_all('p',{\"class\":\"meta\"})\n",
    "    loc=[]\n",
    "    dat=[]\n",
    "    v={}\n",
    "    for om in range(len(video)):\n",
    "        try:\n",
    "            loc.append(video[om].find(\"a\",attrs={'rel':re.compile(\"^nofollow\")}).text)\n",
    "            loc.append(video[om].find(\"a\",attrs={'rel':re.compile(\"^nofollow\")}).get('href'))\n",
    "            loc.append(video1[om].find(\"a\",attrs={'href':re.compile(\"^/web\")}).text)\n",
    "            loc.append(video1[om].find(\"a\",attrs={'href':re.compile(\"^/http\")}).text.replace('\\t','').replace('\\n',' '))\n",
    "        except:\n",
    "            None\n",
    "    dat.append(loc)\n",
    "    v['Title','Url','Event Name',]=dat\n",
    "    d['All Videos']=v\n",
    "    l.append(d)\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(l)\n",
    "df =df[['Speaker','Display Image url','Title','Location','Attended Event','All Videos','Slide Decks','Linkedin','Twitter','Facebook','Website']]\n",
    "df= df.set_index('Speaker')\n",
    "df.to_csv(r\"C:\\Users\\YuSuf AnSari\\Desktop\\Web Scrapping\\Speaker Details.csv\")\n",
    "df\n",
    "df_new = pd.read_csv(r'C:\\Users\\YuSuf AnSari\\Desktop\\Web Scrapping\\Speaker Details.csv')\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\YuSuf AnSari\\Desktop\\Web Scrapping\\Speaker Details In excel.xlsx')\n",
    "df_new.to_excel(writer,index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
